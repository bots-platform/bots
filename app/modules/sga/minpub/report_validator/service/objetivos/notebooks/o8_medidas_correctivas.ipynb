{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_info_columns', 10000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.join(notebook_dir, '..','..','..','..','..','..','..','..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "\n",
    "print(\"notebook dir:\", notebook_dir)\n",
    "print(\"project root:\", project_root)\n",
    "print(\"absolute project root:\", os.path.abspath(project_root))\n",
    "print(\"notebook dir:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.join(notebook_dir, '..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "\n",
    "sys.path.append(r\"\")\n",
    "\n",
    "from utils.logger_config import get_sga_logger\n",
    " \n",
    "logger = get_sga_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_exceptions(func):\n",
    "    \"\"\"\n",
    "    Decorator to log exceptions in a function using the shared 'logger'.\n",
    "    It will also re-raise the exception so that the caller can handle it\n",
    "    appropriately (e.g., fail fast or continue).\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as exc:\n",
    "            logger.error(\n",
    "                f\"Error in function '{func.__name__}': {exc}\",\n",
    "                exc_info=True\n",
    "            )\n",
    "            # Optionally, decide whether to re-raise or swallow the exception.\n",
    "            # Usually best practice is to re-raise so the pipeline can decide what to do:\n",
    "            raise\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_decimal_part(df, column):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame column from float (or numeric string) to a string\n",
    "    by removing the decimal part (i.e. converting 13.5 to \"13\", 12.0 to \"12\").\n",
    "    Non-numeric values are converted to NaN and then to an empty string.\n",
    "    \"\"\"\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: str(int(x)) if pd.notnull(x) else '')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_hhmm(hours_float):\n",
    "    hours = int(hours_float)\n",
    "    minutes = int(round((hours_float - hours)*60))\n",
    "    return f\"{hours}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hhmm(total_seconds):\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    return f\"{hours}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null_values(df, fill_str=\"\", fill_float=0.0, fill_datetime=\"\"):\n",
    "    \"\"\"\n",
    "    Fill null values in DataFrame columns based on data type.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        fill_str (str): Value to replace nulls in object/string columns. Default is \"\".\n",
    "        fill_float (float): Value to replace nulls in float columns. Default is 0.0.\n",
    "        fill_datetime: Value to replace nulls in datetime columns. \n",
    "                       Default is \"\", but you can also pass a default datetime.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with nulls handled.\n",
    "    \"\"\"\n",
    "\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        df[col] = df[col].fillna(fill_str).astype(str)\n",
    "    \n",
    "\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].fillna(fill_float)\n",
    "        \n",
    "\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64[ns]']) \n",
    "    for col in datetime_cols:\n",
    "        df[col] = df[col].fillna(fill_datetime)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resolve_clock_stop_overlaps(clock_stops: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Eliminate overlaps in clock stops (paradas de reloj) by nro_incidencia.\n",
    "\n",
    "    Args:   \n",
    "        clock_stops: List of clock stops with 'start' 'end' datetime and 'nro_incidencia'\n",
    "\n",
    "    Returns:\n",
    "        List of non-overlapping clock stops\n",
    "            \n",
    "    \"\"\"\n",
    "    if not clock_stops:\n",
    "        return []\n",
    "    \n",
    "    incidents = {}\n",
    "    for stop in clock_stops:\n",
    "        nro_incidencia = stop.get('nro_incidencia', 'unknown')\n",
    "        if nro_incidencia not in incidents:\n",
    "            incidents[nro_incidencia] = []\n",
    "        incidents[nro_incidencia].append(stop)\n",
    "\n",
    "    \n",
    "    resolved_all = []   \n",
    "\n",
    "    for nro_incidencia, incident_stops in incidents.items():\n",
    "        sorted_stops = sorted(incident_stops, key=lambda x: x['start'])\n",
    "\n",
    "        for i, stop in enumerate(sorted_stops):\n",
    "            if pd.isna(stop['end']):\n",
    "                if i < len(sorted_stops) - 1 and not pd.isna(sorted_stops[i+1]['start']):\n",
    "                    stop['end'] = sorted_stops[i+1]['start']\n",
    "                else:\n",
    "                    logger.warning(f\"Removing stop with missing end date for nro_incidencia {nro_incidencia}\")\n",
    "                    continue\n",
    "        \n",
    "        valid_stops = [stop for stop in sorted_stops if not pd.isna(stop['end'])]\n",
    "\n",
    "        if not valid_stops:\n",
    "            continue\n",
    "\n",
    "        resolved_stops = [valid_stops[0]]\n",
    "\n",
    "        for current_stop in valid_stops[1:]:\n",
    "            last_resolved = resolved_stops[-1]\n",
    "\n",
    "            if current_stop['start'] <= last_resolved['end']:\n",
    "                last_resolved['end'] = max(last_resolved['end'], current_stop['end'])\n",
    "            else:\n",
    "                resolved_stops.append(current_stop)\n",
    "\n",
    "        resolved_all.extend(resolved_stops)\n",
    "\n",
    "    return resolved_all\n",
    "\n",
    "@log_exceptions\n",
    "def calculate_total_clock_stop_minutes(nro_incidencia:str, interruption_start: datetime, interruption_end: datetime, df_sga_paradas: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the total clock minutes for a ticket, considering constraints.\n",
    "\n",
    "    Args:\n",
    "        nro_incidencia: The ticket identifier\n",
    "        interrupcion_inicio: Start time of the interruption from REPORTE DINAMICO 335 \n",
    "        interrupcion_fin: End time of the interruption from REPORTE DINAMICO 335 \n",
    "    \n",
    "    Returns:\n",
    "        Total clock stop minutes\n",
    "    \n",
    "    \"\"\"   \n",
    "    df_sga_paradas['nro_incidencia'] = df_sga_paradas['nro_incidencia'].astype(str)\n",
    "    nro_incidencia_stops = df_sga_paradas[df_sga_paradas['nro_incidencia'] == nro_incidencia].copy()\n",
    "\n",
    "    if nro_incidencia_stops.empty:\n",
    "        logger.info(f\"No clock stops found for incident {nro_incidencia}\")\n",
    "        return 0.0\n",
    "    \n",
    "    clock_stops = []\n",
    "\n",
    "    for _, stop in nro_incidencia_stops.iterrows():\n",
    "        start_date = stop.get('startdate')\n",
    "        end_date = stop.get('enddate')\n",
    "\n",
    "        if pd.isna(start_date):\n",
    "            logger.warning(f\"Skipping record with missing start date for incident {nro_incidencia}\")\n",
    "            continue\n",
    "\n",
    "        if start_date < interruption_start:\n",
    "            logger.info(f\"Adjusting start time to interruption en for incident {nro_incidencia}\")\n",
    "            start_date = interruption_start\n",
    "\n",
    "        if not pd.isna(end_date):\n",
    "            if end_date > interruption_end:\n",
    "                logger.info(f\"Adjusting end time to interruption en for incident {nro_incidencia}\")\n",
    "                end_date = interruption_end\n",
    "\n",
    "            if start_date < end_date:\n",
    "                clock_stops.append({\n",
    "                    'start': start_date,\n",
    "                    'end': end_date,\n",
    "                    'nro_incidencia': nro_incidencia\n",
    "                })\n",
    "        else:\n",
    "            clock_stops.append({\n",
    "                'start': start_date,\n",
    "                'end': end_date,\n",
    "                'nro_incidencia': nro_incidencia\n",
    "            })\n",
    "    resolved_stops = resolve_clock_stop_overlaps(clock_stops)\n",
    "\n",
    "    total_minutes = sum(\n",
    "        (stop['end'] - stop['start']).total_seconds() / 60\n",
    "        for stop in resolved_stops\n",
    "        if not pd.isna(stop['end']) and not pd.isna(stop['start'])\n",
    "    )\n",
    "    return total_minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_summary(df):\n",
    "    \"\"\"\n",
    "    Returns a summary DataFrame for the given DataFrame.\n",
    "    \n",
    "    The summary includes:\n",
    "      - Data Type\n",
    "      - Non Null Count\n",
    "      - Null Count\n",
    "      - Null Percentage\n",
    "      - Unique Values count\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Data Type': df.dtypes,\n",
    "        'Non Null Count': df.count(),\n",
    "        'Null Count': df.isna().sum(),\n",
    "        'Null Percentage': (df.isna().sum() / len(df) * 100).round(2),\n",
    "        'Unique Values': [df[col].nunique() for col in df.columns],\n",
    "    })\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent.parent.parent.parent.parent.parent.parent.parent\n",
    "SAVE_DIR_EXTRACT_EXCEL = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"excel\"/ \"CORTE 2_20250410_194426.xlsx\"\n",
    "SAVE_DIR_EXTRACT_SGA_335 = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"sga_335\" / \"sga_reporte_30-03-2025_06-04-2025_20250410_173936.xlsx\"\n",
    "CID_CUISMP_PATH = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"sharepoint_cid_cuismp\" / \"MINPU - CID-CUISMP - AB.xlsx\"\n",
    "DIR_PARADAS_RELOJ = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"pausa_cliente\" / \"sga_reporte_30-03-2025_04-04-2025_20250410_195338.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corte_excel = pd.read_excel(SAVE_DIR_EXTRACT_EXCEL, skipfooter=2, engine=\"openpyxl\")\n",
    "df_sga_dinamico_335 = pd.read_excel(SAVE_DIR_EXTRACT_SGA_335) \n",
    "df_sga_dinamico_380 = pd.read_excel(DIR_PARADAS_RELOJ)\n",
    "df_cid_cuismp_sharepoint = pd.read_excel(CID_CUISMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sga_dinamico_335['interrupcion_inicio'] = pd.to_datetime(df_sga_dinamico_335['interrupcion_inicio'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['interrupcion_fin'] = pd.to_datetime(df_sga_dinamico_335['interrupcion_fin'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['fecha_comunicacion_cliente'] = pd.to_datetime(df_sga_dinamico_335['fecha_comunicacion_cliente'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['fecha_generacion'] = pd.to_datetime(df_sga_dinamico_335['fecha_generacion'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['fg_padre'] = pd.to_datetime(df_sga_dinamico_335['fg_padre'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['hora_sistema'] = pd.to_datetime(df_sga_dinamico_335['hora_sistema'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335[\"cid\"] = df_sga_dinamico_335[\"cid\"].astype(str).fillna(\"\")\n",
    "df_sga_dinamico_335['nro_incidencia'] = df_sga_dinamico_335['nro_incidencia'].astype(str)\n",
    "df_sga_dinamico_335 = handle_null_values(df_sga_dinamico_335)\n",
    "df_sga_dinamico_335[\"it_determinacion_de_la_causa\"] = df_sga_dinamico_335[\"it_determinacion_de_la_causa\"].astype(str).str.strip().fillna('No disponible')\n",
    "df_sga_dinamico_335[\"tipo_caso\"] = df_sga_dinamico_335[\"tipo_caso\"].astype(str).str.strip().fillna('No disponible')\n",
    "df_sga_dinamico_335[\"cid\"] = df_sga_dinamico_335[\"cid\"].astype(str).str.strip().fillna('No disponible')\n",
    "df_sga_dinamico_335 = cut_decimal_part(df_sga_dinamico_335, 'codincidencepadre')\n",
    "\n",
    "df_sga_dinamico_380['startdate'] = pd.to_datetime(df_sga_dinamico_380['startdate'],  errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_380['enddate'] = pd.to_datetime(df_sga_dinamico_380['enddate'],  errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_380 = handle_null_values(df_sga_dinamico_380)\n",
    "\n",
    "df_corte_excel = cut_decimal_part(df_corte_excel,'CUISMP')\n",
    "#df_corte_excel = cut_decimal_part(df_corte_excel,'CODINCIDENCEPADRE')\n",
    "df_corte_excel = handle_null_values(df_corte_excel)\n",
    "df_corte_excel = df_corte_excel.rename(columns={'TICKET':'nro_incidencia'})\n",
    "df_corte_excel['nro_incidencia'] = df_corte_excel['nro_incidencia'].astype(str)\n",
    "df_corte_excel['DF'] = df_corte_excel['DF'].astype(str).str.strip().fillna('No disponible').str.lower()\n",
    "df_corte_excel['CUISMP'] = df_corte_excel['CUISMP'].astype(str).str.strip().fillna('No disponible')\n",
    "df_corte_excel['DETERMINACIÓN DE LA CAUSA'] = df_corte_excel['DETERMINACIÓN DE LA CAUSA'].astype(str).str.strip().fillna(\"No disponible\")\n",
    "df_corte_excel['TIPO CASO'] = df_corte_excel['TIPO CASO'].astype(str).str.strip().fillna(\"No disponible\")\n",
    "df_corte_excel['CID'] = df_corte_excel['CID'].astype(str).str.strip().fillna(\"No disponible\")\n",
    "\n",
    "df_cid_cuismp_sharepoint = cut_decimal_part(df_cid_cuismp_sharepoint, 'CUISMP')\n",
    "df_cid_cuismp_sharepoint = df_cid_cuismp_sharepoint.rename(columns={\"CID\":\"cid\"})\n",
    "df_cid_cuismp_sharepoint[\"cid\"] = df_cid_cuismp_sharepoint[\"cid\"].astype(str).fillna(\"\")\n",
    "df_cid_cuismp_sharepoint[\"Distrito Fiscal\"] = df_cid_cuismp_sharepoint[\"Distrito Fiscal\"].astype(str).str.strip().fillna('No disponible').str.lower()\n",
    "df_cid_cuismp_sharepoint[\"CUISMP\"] = df_cid_cuismp_sharepoint[\"CUISMP\"].astype(str).str.strip().fillna('No disponible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sga_335_corte_excel_sharepoint_cuismp_sga380(\n",
    "        df_corte_excel: pd.DataFrame, \n",
    "        df_sga_dinamico_335: pd.DataFrame,\n",
    "        df_cid_cuismp_sharepoint: pd.DataFrame,\n",
    "        df_sga_dinamico_380: pd.DataFrame,\n",
    "        match_type:str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Common merge function for Objective 1.\n",
    "\n",
    "        Merges:\n",
    "          - corte-excel  with sga_dinamico_335 on 'nro_incidencia'\n",
    "\n",
    "        Returns a merged DataFrame with common columns needed.\n",
    "        \"\"\"\n",
    "\n",
    "        merged_sga335_excel = pd.merge(\n",
    "            df_corte_excel,\n",
    "            df_sga_dinamico_335,\n",
    "            on='nro_incidencia',\n",
    "            how='left',\n",
    "            indicator=True,\n",
    "            suffixes=('_corte_excel', '_sga_dinamico_335')\n",
    "        )\n",
    "\n",
    "        merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp = pd.merge(\n",
    "        merged_sga335_excel,\n",
    "        df_cid_cuismp_sharepoint,\n",
    "        on='cid',\n",
    "        how='left',\n",
    "        suffixes=('_sga_dinamico_335_excel_matched', '_sharepoint_cid_cuismp')\n",
    "        )\n",
    "\n",
    "        merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp['sum_paradas'] = merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp.apply(\n",
    "            lambda r: calculate_total_clock_stop_minutes(\n",
    "                nro_incidencia = r[\"nro_incidencia\"],\n",
    "                interruption_start = r[\"interrupcion_inicio\"],\n",
    "                interruption_end = r[\"interrupcion_fin\"],\n",
    "                df_sga_paradas = df_sga_dinamico_380\n",
    "            ),\n",
    "            axis= 1\n",
    "        )\n",
    "\n",
    "        matched_rows = merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp[merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp['_merge'] == match_type]\n",
    "\n",
    "        return matched_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched_corte_sga335_Sharepoint_cuismp_sga380 = merge_sga_335_corte_excel_sharepoint_cuismp_sga380(\n",
    "        df_corte_excel, df_sga_dinamico_335,\n",
    "        df_cid_cuismp_sharepoint, df_sga_dinamico_380,\n",
    "        'both'\n",
    "        )\n",
    "df_unmatched_corte_sga335_Sharepoint_cuismp_sga380 = merge_sga_335_corte_excel_sharepoint_cuismp_sga380(\n",
    "    df_corte_excel,\n",
    "    df_sga_dinamico_335,\n",
    "    df_cid_cuismp_sharepoint,\n",
    "    df_sga_dinamico_380,\n",
    "    'left_only'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched_corte_sga335_Sharepoint_cuismp_sga380.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info =  get_dataframe_summary(df_matched_corte_sga335_Sharepoint_cuismp_sga380)\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@log_exceptions\n",
    "def validation_medidas_correctivas(merged_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validation the column medidas correctivas y o medidas tomadas, se debe obtener\n",
    "    the first and the last date from paragraph, excluding the two last lines if exists \n",
    "    dates called fecha hora inicio and fecha hora fin\n",
    "    \"\"\"\n",
    "\n",
    "    df = merged_df.copy()\n",
    "\n",
    "    df['fechas_parrafos_coinciden'] = True\n",
    "    df['fechas_parrafos_match_columns_first_date'] = True\n",
    "    df['fechas_parrafos_match_columns_last_date'] = True\n",
    "    df['no_errores_ortograficos'] = True\n",
    "\n",
    "    def extract_dates(row):\n",
    "        medidas_text = row['MEDIDAS CORRECTIVAS Y/O PREVENTIVAS TOMADAS']\n",
    "\n",
    "        default = {\n",
    "            'first_date':None,\n",
    "            'last_date': None,\n",
    "            'fecha_inicio': None,\n",
    "            'fecha_fin': None,\n",
    "            'last_date': None,\n",
    "            'spelling_errors': False\n",
    "        }\n",
    "\n",
    "        if not isinstance(medidas_text, str) or not medidas_text.strip():\n",
    "            return default\n",
    "        \n",
    "        lines = medidas_text.split('\\n')\n",
    "        paragraph_lines = []\n",
    "        fecha_inicio_line = \"\"\n",
    "        fecha_fin_line = \"\"\n",
    "\n",
    "        if len(lines) >=2:\n",
    "            if lines[-2].strip().startswith(\"Fecha y hora inicio\"):\n",
    "                fecha_inicio_line = lines[-2]\n",
    "                paragraph_lines = lines[:-2]\n",
    "            if lines[-1].strip().startswith(\"Fecha y hora fin\"):\n",
    "                fecha_fin_line = lines[-1]\n",
    "                if not paragraph_lines:\n",
    "                    paragraph_lines = lines[:-1]\n",
    "\n",
    "        if not paragraph_lines:\n",
    "            paragraph_lines = lines\n",
    "\n",
    "        paragraph = '\\n'.join(paragraph_lines)\n",
    "\n",
    "        import re\n",
    "\n",
    "        datepattern_in_paragraph = r'(?:el )?(?:dia)?(\\d{1,2}/\\d{1,2}/\\d{4})(?:,)? (?:a las)?(\\d{1,2}:\\d{2})(?:\\s?horas)?'\n",
    "        datapattern_last_2_lines = r'\\d{1,2}/\\d{1,2}/\\d{4} \\d{1,2}:\\d{2})'\n",
    "        dates_in_paragraph = re.findall(datepattern_in_paragraph, paragraph)\n",
    "\n",
    "        formatted_dates = []\n",
    "        for date, time in dates_in_paragraph:\n",
    "            date_parts = date.split('/')\n",
    "            if len(date_parts) == 3:\n",
    "                day = date_parts[0].zfill(2)\n",
    "                month = date_parts[1].zfill(2)\n",
    "                year = date_parts[2]\n",
    "                date = f\"{day}/{month}/{year}\"\n",
    "\n",
    "            time_parts = time.split(':')\n",
    "            if len(time_parts) == 2:\n",
    "                hour = time_parts[0].zfill(2)\n",
    "                minute = time_parts[1].zfill(2)\n",
    "                time = f\"{hour}:{minute}\"\n",
    "            \n",
    "            formatted_dates.append(f\"{date} {time}\")\n",
    "\n",
    "        first_date = formatted_dates[0] if formatted_dates else None\n",
    "        last_date = formatted_dates[-1] if len(formatted_dates) > 1 else first_date\n",
    "\n",
    "        fecha_inicio = None\n",
    "        if fecha_inicio_line:   \n",
    "            inicio_match = re.search(datapattern_last_2_lines, fecha_inicio_line)\n",
    "            if inicio_match:\n",
    "                fecha_inicio = inicio_match.group(0)\n",
    "\n",
    "        fecha_fin = None\n",
    "        if fecha_fin_line:\n",
    "            fin_match = re.search(datapattern_last_2_lines, fecha_fin_line)\n",
    "            if fin_match:\n",
    "                fecha_fin = fin_match.group(0)\n",
    "\n",
    "        spelling_errors = False\n",
    "        error_patterns = [\n",
    "            r'inmendiatamente'\n",
    "        ]\n",
    "\n",
    "        for pattern in error_patterns:\n",
    "            if re.search(pattern, paragraph, re.IGNORECASE):\n",
    "                spelling_errors  = True\n",
    "                break\n",
    "        return{\n",
    "            'first_date': first_date,\n",
    "            'last_date': last_date,\n",
    "            'fecha_inicio': fecha_inicio,\n",
    "            'fecha_fin': fecha_fin,\n",
    "            'spelling_errors': spelling_errors,\n",
    "        }\n",
    "    \n",
    "    results = df.apply(extract_dates, axis=1)\n",
    "    results_df = pd.DataFrame(results.tolist(), index=df.index)\n",
    "\n",
    "    df = pd.concat([df,  results_df], axis=1)\n",
    "    \n",
    "    df['fechas_parrafos_match_columns_first_date'] = (\n",
    "        df['first_date'] == df['FECHA Y HORA INICIO']\n",
    "    )\n",
    "\n",
    "    df['fechas_parrafos_match_columns_last_date'] = (\n",
    "        df['last_date'] == df['FECHA Y HORA FIN']\n",
    "    )\n",
    "\n",
    "    df['no_errores_ortograficos'] = ~df['spelling_errors']\n",
    "\n",
    "    df['validation_medidas_correctivas_ok'] = (\n",
    "        df['fechas_parrafos_match_columns_first_date'] &\n",
    "        df['fechas_parrafos_match_columns_last_date'] &\n",
    "        df['no_errores_ortograficos'] \n",
    "    )\n",
    "\n",
    "    df['Validation_OK'] = df['validation_medidas_correctivas_ok']\n",
    "    df['fail_count'] = (~df['Validation_OK']).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
