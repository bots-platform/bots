{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import language_tool_python\n",
    "# _tool_es = language_tool_python.LanguageTool('es')\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_info_columns', 10000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.join(notebook_dir, '..','..','..','..','..','..','..','..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "\n",
    "print(\"notebook dir:\", notebook_dir)\n",
    "print(\"project root:\", project_root)\n",
    "print(\"absolute project root:\", os.path.abspath(project_root))\n",
    "print(\"notebook dir:\", sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.join(notebook_dir, '..')\n",
    "sys.path.append(os.path.abspath(project_root))\n",
    "\n",
    "sys.path.append(r\"\")\n",
    "\n",
    "from utils.logger_config import get_sga_logger\n",
    " \n",
    "logger = get_sga_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_exceptions(func):\n",
    "    \"\"\"\n",
    "    Decorator to log exceptions in a function using the shared 'logger'.\n",
    "    It will also re-raise the exception so that the caller can handle it\n",
    "    appropriately (e.g., fail fast or continue).\n",
    "    \"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as exc:\n",
    "            logger.error(\n",
    "                f\"Error in function '{func.__name__}': {exc}\",\n",
    "                exc_info=True\n",
    "            )\n",
    "            # Optionally, decide whether to re-raise or swallow the exception.\n",
    "            # Usually best practice is to re-raise so the pipeline can decide what to do:\n",
    "            raise\n",
    "    return wrapper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_decimal_part(df, column):\n",
    "    \"\"\"\n",
    "    Converts a DataFrame column from float (or numeric string) to a string\n",
    "    by removing the decimal part (i.e. converting 13.5 to \"13\", 12.0 to \"12\").\n",
    "    Non-numeric values are converted to NaN and then to an empty string.\n",
    "    \"\"\"\n",
    "    df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    df[column] = df[column].apply(lambda x: str(int(x)) if pd.notnull(x) else '')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_hhmm(hours_float):\n",
    "    hours = int(hours_float)\n",
    "    minutes = int(round((hours_float - hours)*60))\n",
    "    return f\"{hours}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_hhmm(total_seconds):\n",
    "    hours = total_seconds // 3600\n",
    "    minutes = (total_seconds % 3600) // 60\n",
    "    return f\"{hours}:{minutes:02d}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_null_values(df, fill_str=\"\", fill_float=0.0, fill_datetime=\"\"):\n",
    "    \"\"\"\n",
    "    Fill null values in DataFrame columns based on data type.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        fill_str (str): Value to replace nulls in object/string columns. Default is \"\".\n",
    "        fill_float (float): Value to replace nulls in float columns. Default is 0.0.\n",
    "        fill_datetime: Value to replace nulls in datetime columns. \n",
    "                       Default is \"\", but you can also pass a default datetime.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with nulls handled.\n",
    "    \"\"\"\n",
    "\n",
    "    obj_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in obj_cols:\n",
    "        df[col] = df[col].fillna(fill_str).astype(str)\n",
    "    \n",
    "\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    for col in float_cols:\n",
    "        df[col] = df[col].fillna(fill_float)\n",
    "        \n",
    "\n",
    "    datetime_cols = df.select_dtypes(include=['datetime64[ns]']) \n",
    "    for col in datetime_cols:\n",
    "        df[col] = df[col].fillna(fill_datetime)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resolve_clock_stop_overlaps(clock_stops: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Eliminate overlaps in clock stops (paradas de reloj) by nro_incidencia.\n",
    "\n",
    "    Args:   \n",
    "        clock_stops: List of clock stops with 'start' 'end' datetime and 'nro_incidencia'\n",
    "\n",
    "    Returns:\n",
    "        List of non-overlapping clock stops\n",
    "            \n",
    "    \"\"\"\n",
    "    if not clock_stops:\n",
    "        return []\n",
    "    \n",
    "    incidents = {}\n",
    "    for stop in clock_stops:\n",
    "        nro_incidencia = stop.get('nro_incidencia', 'unknown')\n",
    "        if nro_incidencia not in incidents:\n",
    "            incidents[nro_incidencia] = []\n",
    "        incidents[nro_incidencia].append(stop)\n",
    "\n",
    "    \n",
    "    resolved_all = []   \n",
    "\n",
    "    for nro_incidencia, incident_stops in incidents.items():\n",
    "        sorted_stops = sorted(incident_stops, key=lambda x: x['start'])\n",
    "\n",
    "        for i, stop in enumerate(sorted_stops):\n",
    "            if pd.isna(stop['end']):\n",
    "                if i < len(sorted_stops) - 1 and not pd.isna(sorted_stops[i+1]['start']):\n",
    "                    stop['end'] = sorted_stops[i+1]['start']\n",
    "                else:\n",
    "                    logger.warning(f\"Removing stop with missing end date for nro_incidencia {nro_incidencia}\")\n",
    "                    continue\n",
    "        \n",
    "        valid_stops = [stop for stop in sorted_stops if not pd.isna(stop['end'])]\n",
    "\n",
    "        if not valid_stops:\n",
    "            continue\n",
    "\n",
    "        resolved_stops = [valid_stops[0]]\n",
    "\n",
    "        for current_stop in valid_stops[1:]:\n",
    "            last_resolved = resolved_stops[-1]\n",
    "\n",
    "            if current_stop['start'] <= last_resolved['end']:\n",
    "                last_resolved['end'] = max(last_resolved['end'], current_stop['end'])\n",
    "            else:\n",
    "                resolved_stops.append(current_stop)\n",
    "\n",
    "        resolved_all.extend(resolved_stops)\n",
    "\n",
    "    return resolved_all\n",
    "\n",
    "@log_exceptions\n",
    "def calculate_total_clock_stop_minutes(nro_incidencia:str, interruption_start: datetime, interruption_end: datetime, df_sga_paradas: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the total clock minutes for a ticket, considering constraints.\n",
    "\n",
    "    Args:\n",
    "        nro_incidencia: The ticket identifier\n",
    "        interrupcion_inicio: Start time of the interruption from REPORTE DINAMICO 335 \n",
    "        interrupcion_fin: End time of the interruption from REPORTE DINAMICO 335 \n",
    "    \n",
    "    Returns:\n",
    "        Total clock stop minutes\n",
    "    \n",
    "    \"\"\"   \n",
    "    df_sga_paradas['nro_incidencia'] = df_sga_paradas['nro_incidencia'].astype(str)\n",
    "    nro_incidencia_stops = df_sga_paradas[df_sga_paradas['nro_incidencia'] == nro_incidencia].copy()\n",
    "\n",
    "    if nro_incidencia_stops.empty:\n",
    "        logger.info(f\"No clock stops found for incident {nro_incidencia}\")\n",
    "        return 0.0\n",
    "    \n",
    "    clock_stops = []\n",
    "\n",
    "    for _, stop in nro_incidencia_stops.iterrows():\n",
    "        start_date = stop.get('startdate')\n",
    "        end_date = stop.get('enddate')\n",
    "\n",
    "        if pd.isna(start_date):\n",
    "            logger.warning(f\"Skipping record with missing start date for incident {nro_incidencia}\")\n",
    "            continue\n",
    "\n",
    "        if start_date < interruption_start:\n",
    "            logger.info(f\"Adjusting start time to interruption en for incident {nro_incidencia}\")\n",
    "            start_date = interruption_start\n",
    "\n",
    "        if not pd.isna(end_date):\n",
    "            if end_date > interruption_end:\n",
    "                logger.info(f\"Adjusting end time to interruption en for incident {nro_incidencia}\")\n",
    "                end_date = interruption_end\n",
    "\n",
    "            if start_date < end_date:\n",
    "                clock_stops.append({\n",
    "                    'start': start_date,\n",
    "                    'end': end_date,\n",
    "                    'nro_incidencia': nro_incidencia\n",
    "                })\n",
    "        else:\n",
    "            clock_stops.append({\n",
    "                'start': start_date,\n",
    "                'end': end_date,\n",
    "                'nro_incidencia': nro_incidencia\n",
    "            })\n",
    "    resolved_stops = resolve_clock_stop_overlaps(clock_stops)\n",
    "\n",
    "    total_minutes = sum(\n",
    "        (stop['end'] - stop['start']).total_seconds() / 60\n",
    "        for stop in resolved_stops\n",
    "        if not pd.isna(stop['end']) and not pd.isna(stop['start'])\n",
    "    )\n",
    "    return total_minutes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe_summary(df):\n",
    "    \"\"\"\n",
    "    Returns a summary DataFrame for the given DataFrame.\n",
    "    \n",
    "    The summary includes:\n",
    "      - Data Type\n",
    "      - Non Null Count\n",
    "      - Null Count\n",
    "      - Null Percentage\n",
    "      - Unique Values count\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Data Type': df.dtypes,\n",
    "        'Non Null Count': df.count(),\n",
    "        'Null Count': df.isna().sum(),\n",
    "        'Null Percentage': (df.isna().sum() / len(df) * 100).round(2),\n",
    "        'Unique Values': [df[col].nunique() for col in df.columns],\n",
    "    })\n",
    "    \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd().parent.parent.parent.parent.parent.parent.parent.parent\n",
    "SAVE_DIR_EXTRACT_EXCEL = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"excel\"/ \"CORTE 2.xlsx\"\n",
    "SAVE_DIR_EXTRACT_SGA_335 = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"sga_335\" / \"sga_reporte_30-03-2025_06-04-2025_20250410_173936.xlsx\"\n",
    "CID_CUISMP_PATH = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"sharepoint_cid_cuismp\" / \"MINPU - CID-CUISMP - AB.xlsx\"\n",
    "DIR_PARADAS_RELOJ = BASE_DIR / \"media\" / \"minpub\" / \"validator_report\" / \"extract\" / \"pausa_cliente\" / \"sga_reporte_30-03-2025_04-04-2025_20250410_195338.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corte_excel = pd.read_excel(SAVE_DIR_EXTRACT_EXCEL, skipfooter=2, engine=\"openpyxl\")\n",
    "df_sga_dinamico_335 = pd.read_excel(SAVE_DIR_EXTRACT_SGA_335) \n",
    "df_sga_dinamico_380 = pd.read_excel(DIR_PARADAS_RELOJ)\n",
    "df_cid_cuismp_sharepoint = pd.read_excel(CID_CUISMP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sga_dinamico_335['interrupcion_inicio'] = pd.to_datetime(df_sga_dinamico_335['interrupcion_inicio'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['interrupcion_fin'] = pd.to_datetime(df_sga_dinamico_335['interrupcion_fin'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['fecha_comunicacion_cliente'] = pd.to_datetime(df_sga_dinamico_335['fecha_comunicacion_cliente'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['fecha_generacion'] = pd.to_datetime(df_sga_dinamico_335['fecha_generacion'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['fg_padre'] = pd.to_datetime(df_sga_dinamico_335['fg_padre'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335['hora_sistema'] = pd.to_datetime(df_sga_dinamico_335['hora_sistema'], errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_335[\"cid\"] = df_sga_dinamico_335[\"cid\"].astype(str).fillna(\"\")\n",
    "df_sga_dinamico_335['nro_incidencia'] = df_sga_dinamico_335['nro_incidencia'].astype(str)\n",
    "df_sga_dinamico_335 = handle_null_values(df_sga_dinamico_335)\n",
    "df_sga_dinamico_335[\"it_determinacion_de_la_causa\"] = df_sga_dinamico_335[\"it_determinacion_de_la_causa\"].astype(str).str.strip().fillna('No disponible')\n",
    "df_sga_dinamico_335[\"tipo_caso\"] = df_sga_dinamico_335[\"tipo_caso\"].astype(str).str.strip().fillna('No disponible')\n",
    "df_sga_dinamico_335[\"cid\"] = df_sga_dinamico_335[\"cid\"].astype(str).str.strip().fillna('No disponible')\n",
    "df_sga_dinamico_335 = cut_decimal_part(df_sga_dinamico_335, 'codincidencepadre')\n",
    "\n",
    "df_sga_dinamico_380['startdate'] = pd.to_datetime(df_sga_dinamico_380['startdate'],  errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_380['enddate'] = pd.to_datetime(df_sga_dinamico_380['enddate'],  errors='coerce', dayfirst=True)\n",
    "df_sga_dinamico_380 = handle_null_values(df_sga_dinamico_380)\n",
    "\n",
    "df_corte_excel = cut_decimal_part(df_corte_excel,'CUISMP')\n",
    "#df_corte_excel = cut_decimal_part(df_corte_excel,'CODINCIDENCEPADRE')\n",
    "df_corte_excel = handle_null_values(df_corte_excel)\n",
    "df_corte_excel = df_corte_excel.rename(columns={'TICKET':'nro_incidencia'})\n",
    "df_corte_excel['nro_incidencia'] = df_corte_excel['nro_incidencia'].astype(str)\n",
    "df_corte_excel['DF'] = df_corte_excel['DF'].astype(str).str.strip().fillna('No disponible').str.lower()\n",
    "df_corte_excel['CUISMP'] = df_corte_excel['CUISMP'].astype(str).str.strip().fillna('No disponible')\n",
    "df_corte_excel['DETERMINACIÓN DE LA CAUSA'] = df_corte_excel['DETERMINACIÓN DE LA CAUSA'].astype(str).str.strip().fillna(\"No disponible\")\n",
    "df_corte_excel['TIPO CASO'] = df_corte_excel['TIPO CASO'].astype(str).str.strip().fillna(\"No disponible\")\n",
    "df_corte_excel['CID'] = df_corte_excel['CID'].astype(str).str.strip().fillna(\"No disponible\")\n",
    "\n",
    "df_cid_cuismp_sharepoint = cut_decimal_part(df_cid_cuismp_sharepoint, 'CUISMP')\n",
    "df_cid_cuismp_sharepoint = df_cid_cuismp_sharepoint.rename(columns={\"CID\":\"cid\"})\n",
    "df_cid_cuismp_sharepoint[\"cid\"] = df_cid_cuismp_sharepoint[\"cid\"].astype(str).fillna(\"\")\n",
    "df_cid_cuismp_sharepoint[\"Distrito Fiscal\"] = df_cid_cuismp_sharepoint[\"Distrito Fiscal\"].astype(str).str.strip().fillna('No disponible').str.lower()\n",
    "df_cid_cuismp_sharepoint[\"CUISMP\"] = df_cid_cuismp_sharepoint[\"CUISMP\"].astype(str).str.strip().fillna('No disponible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sga_335_corte_excel_sharepoint_cuismp_sga380(\n",
    "        df_corte_excel: pd.DataFrame, \n",
    "        df_sga_dinamico_335: pd.DataFrame,\n",
    "        df_cid_cuismp_sharepoint: pd.DataFrame,\n",
    "        df_sga_dinamico_380: pd.DataFrame,\n",
    "        match_type:str\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Common merge function for Objective 1.\n",
    "\n",
    "        Merges:\n",
    "          - corte-excel  with sga_dinamico_335 on 'nro_incidencia'\n",
    "\n",
    "        Returns a merged DataFrame with common columns needed.\n",
    "        \"\"\"\n",
    "\n",
    "        merged_sga335_excel = pd.merge(\n",
    "            df_corte_excel,\n",
    "            df_sga_dinamico_335,\n",
    "            on='nro_incidencia',\n",
    "            how='left',\n",
    "            indicator=True,\n",
    "            suffixes=('_corte_excel', '_sga_dinamico_335')\n",
    "        )\n",
    "\n",
    "        merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp = pd.merge(\n",
    "        merged_sga335_excel,\n",
    "        df_cid_cuismp_sharepoint,\n",
    "        on='cid',\n",
    "        how='left',\n",
    "        suffixes=('_sga_dinamico_335_excel_matched', '_sharepoint_cid_cuismp')\n",
    "        )\n",
    "\n",
    "        merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp['sum_paradas'] = merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp.apply(\n",
    "            lambda r: calculate_total_clock_stop_minutes(\n",
    "                nro_incidencia = r[\"nro_incidencia\"],\n",
    "                interruption_start = r[\"interrupcion_inicio\"],\n",
    "                interruption_end = r[\"interrupcion_fin\"],\n",
    "                df_sga_paradas = df_sga_dinamico_380\n",
    "            ),\n",
    "            axis= 1\n",
    "        )\n",
    "\n",
    "        matched_rows = merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp[merge_sga_335_corte_excel_matched_with_sharepoint_cid_cuismp['_merge'] == match_type]\n",
    "\n",
    "        return matched_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched_corte_sga335_Sharepoint_cuismp_sga380 = merge_sga_335_corte_excel_sharepoint_cuismp_sga380(\n",
    "        df_corte_excel, df_sga_dinamico_335,\n",
    "        df_cid_cuismp_sharepoint, df_sga_dinamico_380,\n",
    "        'both'\n",
    "        )\n",
    "df_unmatched_corte_sga335_Sharepoint_cuismp_sga380 = merge_sga_335_corte_excel_sharepoint_cuismp_sga380(\n",
    "    df_corte_excel,\n",
    "    df_sga_dinamico_335,\n",
    "    df_cid_cuismp_sharepoint,\n",
    "    df_sga_dinamico_380,\n",
    "    'left_only'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched_corte_sga335_Sharepoint_cuismp_sga380.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info =  get_dataframe_summary(df_matched_corte_sga335_Sharepoint_cuismp_sga380)\n",
    "info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def validate_responsable(df_merged: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validates that RESPONSABLE (EXCEL-CORTE) equals the first word of tipificacion_tipo (SGA 335)\n",
    "    \n",
    "    Adds:\n",
    "        - responsable_expected: first word of tipificacion_tipo\n",
    "        - responsable_ok: True/False comparison\n",
    "        - Validation_OK: AND-combined with any existing flag\n",
    "        - fail_count: 0/1\n",
    "    \"\"\"\n",
    "    df = df_merged.copy()\n",
    "\n",
    "    df['responsable_expected'] = (\n",
    "        df['tipificacion_tipo']\n",
    "        .astype(str)\n",
    "        .str.split('-', n=1)\n",
    "        .str[0]\n",
    "        .str.strip()\n",
    "    )\n",
    "    df['responsable_OK'] = (\n",
    "        df['RESPONSABILIDAD'].astype(str).str.strip() == df['responsable_expected']\n",
    "    )\n",
    "\n",
    "    df['Validation_OK'] = df['responsable_OK']\n",
    "    df['fail_count'] = (~df['Validation_OK']).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_validation = validate_responsable(df_matched_corte_sga335_Sharepoint_cuismp_sga380)\n",
    "df_validation.head(1)\n",
    "prueba = df_validation[df_validation['nro_incidencia'] == '21790461']\n",
    "prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_failure_messages_responsable(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds specific failure messages for the RESPONSABILIDAD vs tipificacion_tipo check.\n",
    "    Returns a Dataframe with columns ['ID', 'mensaje', 'objetivo'] for failing records only.\n",
    "    \"\"\"\n",
    "\n",
    "    if df.empty or 'Validation_OK' not in df.columns:\n",
    "        return pd.DataFrame(columns=['nro_incidencia', 'mensaje', 'TIPO REPORTE','objetivo'])\n",
    "    \n",
    "    messages = np.where(\n",
    "        df['Validation_OK'],\n",
    "        \"Validation exitosa : RESPONSABLE coincide  con la primera palabra de tipificacion_tipo\",\n",
    "        (\n",
    "            \"RESPONSABILIDAD: \" \n",
    "            + df[\"RESPONSABILIDAD\"].astype(str)\n",
    "            + \" ' no coincide con la primera palabra' \"\n",
    "            + df['responsable_expected'].astype(str)\n",
    "            +\"' de tipificacion_tipo'.\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df['mensaje'] = messages\n",
    "    df['objetivo'] = 1.9\n",
    "    df_failures = df[df['fail_count'] > 0]\n",
    "\n",
    "    return df_failures[['nro_incidencia', 'mensaje', 'TIPO REPORTE','objetivo']]\n",
    "\n",
    "df_failures = build_failure_messages_responsable(df_validation)\n",
    "df_failures\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
